{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Used\n",
    "\n",
    "https://omkarpathak.in/2018/12/18/writing-your-own-resume-parser/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        \n",
    "        # iterate over all pages of PDF document\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            \n",
    "            # creating a resoure manager\n",
    "            resource_manager = PDFResourceManager()\n",
    "            \n",
    "            # create a file handle\n",
    "            fake_file_handle = io.StringIO()\n",
    "            \n",
    "            # creating a text converter object\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "\n",
    "            # creating a page interpreter\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "\n",
    "            # process current page\n",
    "            page_interpreter.process_page(page)\n",
    "            \n",
    "            # extract text\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "\n",
    "            # close open handles\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n",
    "# calling above function and extracting text\n",
    "text = ''\n",
    "for page in extract_text_from_pdf('Whitmore-resume.pdf'):\n",
    "    text += ' ' + page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Jonathan Whitmore\\nPhD, Data Scientist\\n\\nExperience\\n\\nMountain View, CA\\n+1 650-943-3715\\n(cid:66) JBWhit@gmail.com\\n(cid:205) JonathanWhitmore.com\\nJBWhit\\nJonathanBWhitmore\\n\\n2014-\\nPresent\\n\\nData Scientist, Silicon Valley Data Science, Mountain View, CA, USA.\\n(cid:123) Consulting as a member of several small data science/data engineering teams at multiple companies.\\n(cid:123) Creating output to explain data analysis, data visualization, and statistical modeling results to managers.\\n(cid:123) Modeling survey data responses with ordinal logistic regression in R.\\n(cid:123) Analyzing and visualizing user behavior migration.\\n\\n2014 Insight Fellow, Insight Data Science, Palo Alto, CA, USA.\\n\\n(cid:123) Created a Data Science project to predict the auction sale price of Abstract Expressionist art.\\n\\n2011–2014 Postdoctoral Research Associate, Swinburne University, Melbourne, AUS.\\n\\n(cid:123) Cleaned noisy and inhomogeneous astronomical data taken over four years by diﬀerent observing groups.\\n(cid:123) Curated central data repository of ﬁnal products; developed an automated process to update data\\n\\nrepository; created web interface for collaborator access to the repository.\\n\\n(cid:123) Utilized numerous statistical techniques, including sensitivity analysis on non-linear propagation of\\nerrors, Markov-Chain Monte Carlo for model building, and hypothesis testing via information criterion.\\n(cid:123) Simulated spectroscopic data to expose systematic errors that challenge long-standing results on whether\\n\\nthe fundamental physical constants of the universe are constant.\\n\\n2005–2011 Graduate Student Researcher, UCSD, San Diego, CA, USA.\\n\\n(cid:123) Developed a novel technique to extract information from high resolution spectroscopic data that led to\\n\\nuncovering unknown short-range systematic errors.\\n\\nProgramming and Development Skills\\n\\nLanguages Python, SQL (Impala/Hive), R, LATEX, shell scripts, CSS, HTML.\\n\\nTools Jupyter Notebook, pandas, matplotlib, numpy, scikit-learn, scipy, pymc3, git, pandoc.\\n\\nPublishing, Speaking, and Side Projects\\n\\n2016 O’Reilly author: Jupyter Notebook for Data Science Teams [screencast], editor O’Reilly Media.\\n2016 UC Berkeley Guest Lecturer: Master in Data Science lecture on Jupyter Notebook.\\n2015 Open Source Speaker: OSCON.\\n\\n2014–2015 Technical reviewer of Mastering SciPy by Francisco J. Blanco-Silva, 2015.\\n2012–2014 Developer of RebalanceAssetAllocation, a Python module that recommends ﬁnancial asset class allocations.\\n2013-2014 Contributor to astropy; creator of dipole_error, an astronomy Python module.\\n\\n2013 Co-star and narrator of Hidden Universe, a 3D IMAX astronomy ﬁlm playing worldwide.\\n\\n2007–2009 Graduate Physics Courses Taken: Stochastic Methods, Computational Physics.\\n\\nEducation\\n\\n2011 PhD Physics, University of California San Diego, San Diego, CA, USA.\\n2007 MS Physics, University of California San Diego, San Diego, CA, USA.\\n\\n2005 Bachelor of Science–Magna Cum Laude, Vanderbilt University, Nashville, TN, USA.\\n\\nTriple major: Physics (honors); Mathematics; Philosophy.\\n\\n\\x0c'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', None, pattern)\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jonathan Whitmore\n"
     ]
    }
   ],
   "source": [
    "name = extract_name(text)\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Phone Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+16509433715\n"
     ]
    }
   ],
   "source": [
    "mobile_number = extract_mobile_number(text)\n",
    "print(mobile_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JBWhit@gmail.com\n"
     ]
    }
   ],
   "source": [
    "email = extract_email(text)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Skills\n",
    "First we created a skills.csv file containing some skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"new_skills.csv\", encoding='latin-1') \n",
    "    \n",
    "    # extract values\n",
    "    skills = list(data['skills'])\n",
    "    \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stochastic methods', 'Astronomy', 'Building', 'Silicon', 'Testing', 'Publishing', 'Star', 'Mathematics', 'Git', 'Visualization', 'Physics', 'Allocations', 'Output', 'Alto', 'Research', 'Narrator', 'Data analysis', 'Survey', 'Web', 'Python', 'Sql', 'Scipy', 'Pandas', 'Mastering', 'Modeling', '3d', 'Latex', 'Sensitivity analysis', 'Model building', 'Css', 'Groups', 'Teams', 'Html', 'Numpy', 'Art', 'Philosophy', 'Data visualization', 'Hive', 'Languages', 'Range', 'R', 'Led', 'Repository', 'Project', 'Sensitivity', 'Access', 'Computational physics', 'Matplotlib']\n"
     ]
    }
   ],
   "source": [
    "skills = extract_skills(text)\n",
    "print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    \n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MS', '2005')]\n"
     ]
    }
   ],
   "source": [
    "education = extract_education(text)\n",
    "print(education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Jonathan Whitmore', 'education': [('MS', '2005')], 'email': 'JBWhit@gmail.com', 'mobile_number': '+16509433715', 'skills': ['Stochastic methods', 'Astronomy', 'Building', 'Silicon', 'Testing', 'Publishing', 'Star', 'Mathematics', 'Git', 'Visualization', 'Physics', 'Allocations', 'Output', 'Alto', 'Research', 'Narrator', 'Data analysis', 'Survey', 'Web', 'Python', 'Sql', 'Scipy', 'Pandas', 'Mastering', 'Modeling', '3d', 'Latex', 'Sensitivity analysis', 'Model building', 'Css', 'Groups', 'Teams', 'Html', 'Numpy', 'Art', 'Philosophy', 'Data visualization', 'Hive', 'Languages', 'Range', 'R', 'Led', 'Repository', 'Project', 'Sensitivity', 'Access', 'Computational physics', 'Matplotlib']}\n"
     ]
    }
   ],
   "source": [
    "result = {'name': name, 'education': education,'email': email, 'mobile_number': mobile_number, 'skills': skills}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "results = []\n",
    "results.append(result)\n",
    "\n",
    "filename = 'Jonathan_Whitmore_Resume.csv'\n",
    "with open(filename,'w',newline='') as f:\n",
    "    w = csv.DictWriter(f,list(result.keys()))\n",
    "    w.writeheader()\n",
    "    for r in results:\n",
    "        w.writerow(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>education</th>\n",
       "      <th>email</th>\n",
       "      <th>mobile_number</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jonathan Whitmore</td>\n",
       "      <td>[('MS', '2005')]</td>\n",
       "      <td>JBWhit@gmail.com</td>\n",
       "      <td>16509433715</td>\n",
       "      <td>['Stochastic methods', 'Astronomy', 'Building'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name         education             email  mobile_number  \\\n",
       "0  Jonathan Whitmore  [('MS', '2005')]  JBWhit@gmail.com    16509433715   \n",
       "\n",
       "                                              skills  \n",
       "0  ['Stochastic methods', 'Astronomy', 'Building'...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Jonathan_Whitmore_Resume.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
